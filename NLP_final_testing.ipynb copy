{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "#from sklearn.cluster import KMeans\n",
    "#import gensim\n",
    "\n",
    "consumer_key = \"FazTfC13yURxjIQ1OO42HwaNf\"\n",
    "consumer_secret = \"U5WwW43OmDtTa9JvIriMQAO2c2twxg67eqcEo2cUJsgRc2RmMe\"\n",
    "access_token = \"1482321217-GefE6Iij72ODr9CfuywQX5JfYkS49dUeJvU9JIW\"\n",
    "access_token_secret = \"uA6wULQxiLye9DIAhy0STKaRJhWvrtT2pG4PqryBXg8sb\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "public_tweets = api.home_timeline()\n",
    "private_tweets = api.user_timeline()\n",
    "# Romney_tweets = api.user_timeline(screen_name = \"SenatorRomney\", count = 1000, include_rts = False, tweet_mode = \"extended\")\n",
    "# Malkin_tweets = api.user_timeline(screen_name = \"michellemalkin\", count = 1000, include_rts = False, tweet_mode = \"extended\")\n",
    "# Boehner_tweets = api.user_timeline(screen_name = \"SpeakerBoehner\", count = 1000, include_rts = False, tweet_mode = \"extended\")\n",
    "# Hannity_tweets = api.user_timeline(screen_name = \"seanhannity\", count = 1000, include_rts = False, tweet_mode = \"extended\")\n",
    "# Trump_tweets = api.user_timeline(screen_name = \"realDonaldTrump\", count = 1000, include_rts = False, tweet_mode = \"extended\")\n",
    "# Warren_tweets = api.user_timeline(screen_name = \"ewarren\", count = 1000, include_rts = False, tweet_mode = \"extended\")\n",
    "# Obama_tweets = api.user_timeline(screen_name = \"BarackObama\", count = 1000, include_rts = False, tweet_mode = \"extended\")\n",
    "# Pelosi_tweets = api.user_timeline(screen_name = \"SpeakerPelosi\", count = 1000, include_rts = False, tweet_mode = \"extended\")\n",
    "# Sanders_tweets = api.user_timeline(screen_name = \"BernieSanders\", count = 1000, include_rts = False, tweet_mode = \"extended\")\n",
    "# Biden_tweets = api.user_timeline(screen_name = \"JoeBiden\", count = 1000, include_rts = False, tweet_mode = \"extended\")\n",
    "# print(\"Finished getting all the tweets from the 10 politicans\")\n",
    "\n",
    "tweet_list = []\n",
    "repub_list = []\n",
    "demo_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished adding all republican politican tweets to repub_list\n",
      "Republican Average Sentiment is: 0.08727980725259703\n",
      "Republican Average Subjectivity is: 0.2589273765419354\n"
     ]
    }
   ],
   "source": [
    "# file = open(\"republican_tweet_database.txt\", \"w\")\n",
    "# file_R = open(\"Romney_tweet_database.txt\", \"w\")\n",
    "# file_M = open(\"Malkin_tweet_database.txt\", \"w\")\n",
    "# file_B = open(\"Boehner_tweet_database.txt\", \"w\")\n",
    "# file_H = open(\"Hannity_tweet_database.txt\", \"w\")\n",
    "# file_T = open(\"Trump_tweet_database.txt\", \"w\")\n",
    "# for tweet in Romney_tweets:\n",
    "#     tweet_list.append(tweet.full_text)\n",
    "#     repub_list.append(tweet.full_text)\n",
    "#     file_R.write(tweet.full_text)\n",
    "# for tweet in Malkin_tweets:\n",
    "#     tweet_list.append(tweet.full_text)\n",
    "#     repub_list.append(tweet.full_text)\n",
    "#     file_M.write(tweet.full_text)\n",
    "# for tweet in Boehner_tweets:\n",
    "#     tweet_list.append(tweet.full_text)\n",
    "#     repub_list.append(tweet.full_text)\n",
    "#     file_B.write(tweet.full_text)\n",
    "# for tweet in Hannity_tweets:\n",
    "#     tweet_list.append(tweet.full_text)\n",
    "#     repub_list.append(tweet.full_text)\n",
    "#     file_H.write(tweet.full_text)\n",
    "# for tweet in Trump_tweets:\n",
    "#     tweet_list.append(tweet.full_text)\n",
    "#     repub_list.append(tweet.full_text)\n",
    "#     file_T.write(tweet.full_text)\n",
    "\n",
    "# file_R.close()\n",
    "# file_M.close()\n",
    "# file_B.close()\n",
    "# file_H.close()\n",
    "# file_T.close()\n",
    "\n",
    "# for tweet in repub_list:\n",
    "#     file.write(tweet)\n",
    "#     file.write(\"\\n\")\n",
    "# file.close()\n",
    "file = open(\"text_files/republican_tweet_database.txt\",\"r\")\n",
    "print(\"Finished adding all republican politican tweets to repub_list\")\n",
    "repub_senti_avg = 0\n",
    "repub_subject_avg = 0\n",
    "count = 0\n",
    "for tweet in file:\n",
    "    tweeter = TextBlob(str(tweet))\n",
    "    tweet_senti = tweeter.sentiment.polarity\n",
    "    tweet_subject = tweeter.sentiment.subjectivity\n",
    "    repub_senti_avg += tweet_senti\n",
    "    repub_subject_avg += tweet_subject\n",
    "    count += 1\n",
    "\n",
    "repub_senti_avg = repub_senti_avg / count\n",
    "print(\"Republican Average Sentiment is:\", repub_senti_avg)\n",
    "repub_subject_avg = repub_subject_avg / count\n",
    "print(\"Republican Average Subjectivity is:\", repub_subject_avg)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished adding all democratic politican tweets to demo_list\n",
      "Democratic Average Sentiment is: 0.10272385982675172\n",
      "Democratic Average Subjectivity is: 0.27903661073743363\n"
     ]
    }
   ],
   "source": [
    "# file = open(\"democrat_tweet_database.txt\", \"w\")\n",
    "# file_W = open(\"Warren_tweet_database.txt\", \"w\")\n",
    "# file_O = open(\"Obama_tweet_database.txt\", \"w\")\n",
    "# file_P = open(\"Pelosi_tweet_database.txt\", \"w\")\n",
    "# file_S = open(\"Sanders_tweet_database.txt\", \"w\")\n",
    "# file_B = open(\"Biden_tweet_database.txt\", \"w\")\n",
    "# for tweet in Warren_tweets:\n",
    "#     tweet_list.append(tweet.full_text)\n",
    "#     demo_list.append(tweet.full_text)\n",
    "#     file_W.write(tweet.full_text)\n",
    "# for tweet in Obama_tweets:\n",
    "#     tweet_list.append(tweet.full_text)\n",
    "#     demo_list.append(tweet.full_text)\n",
    "#     file_O.write(tweet.full_text)\n",
    "# for tweet in Pelosi_tweets:\n",
    "#     tweet_list.append(tweet.full_text)\n",
    "#     demo_list.append(tweet.full_text)\n",
    "#     file_P.write(tweet.full_text)\n",
    "# for tweet in Sanders_tweets:\n",
    "#     tweet_list.append(tweet.full_text)\n",
    "#     demo_list.append(tweet.full_text)\n",
    "#     file_S.write(tweet.full_text)\n",
    "# for tweet in Biden_tweets:\n",
    "#     tweet_list.append(tweet.full_text)\n",
    "#     demo_list.append(tweet.full_text)\n",
    "#     file_B.write(tweet.full_text)\n",
    "    \n",
    "# file_W.close()\n",
    "# file_O.close()\n",
    "# file_P.close()\n",
    "# file_S.close()\n",
    "# file_B.close()\n",
    "\n",
    "# for tweet in demo_list:\n",
    "#     file.write(tweet)\n",
    "#     file.write(\"\\n\")\n",
    "# file.close()\n",
    "file = open(\"text_files/democrat_tweet_database.txt\",\"r\")    \n",
    "print(\"Finished adding all democratic politican tweets to demo_list\")\n",
    "demo_senti_avg = 0\n",
    "demo_subject_avg = 0\n",
    "count = 0\n",
    "for tweet in file:\n",
    "    tweeter = TextBlob(str(tweet))\n",
    "    tweet_senti = tweeter.sentiment.polarity\n",
    "    tweet_subject = tweeter.sentiment.subjectivity\n",
    "    demo_senti_avg += tweet_senti\n",
    "    demo_subject_avg += tweet_subject\n",
    "    count +=1\n",
    "    \n",
    "\n",
    "demo_senti_avg = demo_senti_avg / count\n",
    "print(\"Democratic Average Sentiment is:\", demo_senti_avg)\n",
    "demo_subject_avg = demo_subject_avg / count\n",
    "print(\"Democratic Average Subjectivity is:\", demo_subject_avg)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the number of Democratic indepent words is below and can see the words and their number of counts\n",
      "4832\n",
      "[[0.0343, 'We'], [0.0275, 'people'], [0.0224, 'us'], [0.02, 'country'], [0.0196, 'fight'], [0.0187, 'Trump'], [0.0178, 'make'], [0.0172, 'one'], [0.0147, 'I’m'], [0.0145, 'need'], [0.0145, 'must'], [0.0145, 'president'], [0.0145, 'work'], [0.0136, '#DemDebate'], [0.0134, 'American'], [0.013, 'Americans'], [0.013, 'time'], [0.0128, 'take'], [0.0128, 'America'], [0.0128, 'stand'], [0.0125, 'care'], [0.0125, 'This'], [0.0123, '—'], [0.0121, 'plan'], [0.0121, 'world'], [0.0121, 'House'], [0.0119, 'health'], [0.0117, 'like'], [0.0117, 'President'], [0.0117, 'every'], [0.0114, 'change'], [0.0112, 'rights'], [0.0112, 'get'], [0.011, 'going'], [0.0108, 'families'], [0.0106, 'women'], [0.0099, 'right'], [0.0099, 'Donald'], [0.0099, '@AyannaPressley'], [0.0097, 'day'], [0.0092, 'together'], [0.009, 'end'], [0.009, 'power'], [0.0086, 'first'], [0.0086, 'proud'], [0.0086, 'protect'], [0.0084, 'If'], [0.0081, 'When'], [0.0081, 'workers'], [0.0081, 'In'], [0.0081, 'violence'], [0.0079, 'As'], [0.0079, 'vote'], [0.0079, \"I'm\"], [0.0079, 'fighting'], [0.0077, 'future'], [0.0077, 'today'], [0.0077, 'years'], [0.0077, 'want'], [0.0077, 'help'], [0.0075, 'young'], [0.0075, 'history'], [0.0075, 'across'], [0.0075, 'Today'], [0.0075, 'campaign'], [0.0073, 'Black'], [0.0073, 'great'], [0.0073, 'year'], [0.007, 'working'], [0.007, 'support'], [0.007, 'better'], [0.007, 'family'], [0.007, 'live'], [0.007, 'White'], [0.0068, 'white'], [0.0068, 'ensure'], [0.0068, 'Our'], [0.0068, 'It’s'], [0.0066, 'law'], [0.0066, 'big'], [0.0066, 'build'], [0.0066, 'win'], [0.0064, 'climate'], [0.0064, 'US'], [0.0064, 'nation'], [0.0064, 'know'], [0.0064, 'union'], [0.0062, 'got'], [0.0062, 'justice'], [0.0062, 'join'], [0.0059, 'All'], [0.0059, 'leaders'], [0.0059, 'always'], [0.0059, 'communities'], [0.0057, 'way'], [0.0057, 'it’s'], [0.0057, 'see'], [0.0055, 'class'], [0.0055, 'public'], [0.0055, 'Act'], [0.0055, 'life']]\n"
     ]
    }
   ],
   "source": [
    "#Getting Word Lists For Topic Modeling For Democrats\n",
    "file = open(\"text_files/democrat_tweet_database.txt\",\"r\")\n",
    "data_list = []\n",
    "num_list = []\n",
    "stoplist = stopwords.words('english')\n",
    "delimeters = \",./)(:;\\n-!\"\n",
    "otherTokens= [\"yes\", \"okay\", \"ok\", \",\", \";\", \":\", \".\", \"?\", '”', \"--\", \"!\", \"I\", \"n't\", \"would\", \"could\", '“',\n",
    "             \"Dr.\", \"Mrs.\", \"And\", \"But\", \"Miss\", \"'s\", \"It\", \"The\", \"He\", \"You\", \"/\", \")\", \"(\", \"\\n\", \"\", \"&amp\"]\n",
    "for x in range(len(otherTokens)):\n",
    "    stoplist.append(otherTokens[x])\n",
    "for tweet in file:\n",
    "    for i in delimeters: \n",
    "        if i ==\"\\n\" or i == \"-\":\n",
    "            tweet = tweet.replace(i , \" \")\n",
    "        else: \n",
    "            tweet = tweet.replace(i , \"\")\n",
    "    list_tweet_words= tweet.split(\" \")\n",
    "    for i in list_tweet_words:\n",
    "        if i in data_list:\n",
    "            num_list[data_list.index(i)]+=1\n",
    "        else:\n",
    "            if \"http\" not in i and i not in stoplist:\n",
    "                data_list.append(i)\n",
    "                num_list.append(1)\n",
    "demo_tweet_list = []\n",
    "for i in range(len(data_list)):\n",
    "    demo_tweet_list.append([data_list[i], num_list[i]])\n",
    "#print(demo_tweet_list)\n",
    "print(\"The length of the number of Democratic indepent words is below and can see the words and their number of counts\")\n",
    "print(len(demo_tweet_list))\n",
    "max_demo_list = sorted(num_list)[-101:]\n",
    "percentage_demo = []\n",
    "demo_list_sum = sum(max_demo_list)\n",
    "max_use_demo_list = []\n",
    "while len(max_demo_list) != 0:\n",
    "    for e in demo_tweet_list:\n",
    "        if e[1] == max_demo_list[-1]:\n",
    "            percentage_demo.append(round(max_demo_list[-1]/demo_list_sum,4))\n",
    "            max_use_demo_list.append(e[0])\n",
    "            max_demo_list = max_demo_list[:-1]\n",
    "            if len(max_demo_list) == 0:\n",
    "                break\n",
    "# print(max_use_demo_list)\n",
    "demo_max_words_weight_list = []\n",
    "for i in range(len(percentage_demo)):\n",
    "    demo_max_words_weight_list.append([percentage_demo[i],max_use_demo_list[i]])\n",
    "print(demo_max_words_weight_list)\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the number of Republican indepent words is below and can see the words and their number of counts\n",
      "5231\n",
      "[[0.025, 'people'], [0.0249, 'US'], [0.0216, 'This'], [0.02, 'Trump'], [0.0188, 'Utah'], [0.0183, 'great'], [0.0177, 'We'], [0.0172, 'Thank'], [0.0161, 'House'], [0.0155, 'Impeachment'], [0.0144, 'work'], [0.0144, 'want'], [0.0144, '#openbordersinc'], [0.0139, 'President'], [0.0139, 'country'], [0.0139, 'today'], [0.0133, 'must'], [0.0133, 'one'], [0.0127, 'My'], [0.0127, 'American'], [0.0116, 'I’m'], [0.0116, '@RobSchneider'], [0.0116, 'vaping'], [0.0111, 'years'], [0.0111, 'Schiff'], [0.0111, 'Democrats'], [0.0105, 'public'], [0.0105, '@dcexaminer'], [0.01, 'us'], [0.01, 'keep'], [0.01, 'state'], [0.01, 'like'], [0.01, 'young'], [0.01, 'know'], [0.0094, 'back'], [0.0094, 'right'], [0.0094, 'new'], [0.0094, 'last'], [0.0094, 'hearing'], [0.0094, 'need'], [0.0094, 'A'], [0.0094, 'Syria'], [0.0094, 'Americans'], [0.0089, 'Thanks'], [0.0089, 'get'], [0.0089, 'day'], [0.0089, 'As'], [0.0083, 'first'], [0.0083, 'friend'], [0.0083, 'debt'], [0.0083, 'security'], [0.0083, 'Iran'], [0.0083, 'service'], [0.0083, 'America'], [0.0083, 'forward'], [0.0078, 'colleagues'], [0.0078, 'crisis'], [0.0078, 'Utahns'], [0.0078, 'many'], [0.0078, 'future'], [0.0078, 'decision'], [0.0078, 'In'], [0.0078, 'town'], [0.0078, 'go'], [0.0078, 'best'], [0.0078, 'protect'], [0.0078, 'support'], [0.0078, 'national'], [0.0078, 'working'], [0.0078, 'time'], [0.0078, 'important'], [0.0078, 'Utah’s'], [0.0072, 'well'], [0.0072, 'help'], [0.0072, 'It’s'], [0.0072, 'workers'], [0.0072, '1'], [0.0072, 'good'], [0.0072, 'thing'], [0.0072, 'Great'], [0.0072, 'No'], [0.0072, '#AmericaFirst'], [0.0072, 'Speaker'], [0.0072, 'Biden'], [0.0072, 'Congress'], [0.0067, 'health'], [0.0067, 'news'], [0.0067, 'address'], [0.0067, 'going'], [0.0067, 'family'], [0.0067, 'much'], [0.0067, 'allies'], [0.0067, 'year'], [0.0067, 'high'], [0.0067, 'night'], [0.0067, '@Apple'], [0.0067, '@sanctuary_no'], [0.0067, '@JessicaV_CIS'], [0.0067, 'Boston'], [0.0067, 'impeachment'], [0.0067, 'Ukraine']]\n"
     ]
    }
   ],
   "source": [
    "#Getting Word Lists For Topic Modeling For Democrats\n",
    "file = open(\"text_files/republican_tweet_database.txt\",\"r\")\n",
    "data_list = []\n",
    "num_list = []\n",
    "stoplist = stopwords.words('english')\n",
    "otherTokens= [\"yes\", \"okay\", \"ok\", \",\", \";\", \":\", \".\", \"?\", '”', \"--\", \"!\", \"I\", \"n't\", \"would\", \"could\", '“',\n",
    "             \"Dr.\", \"Mrs.\", \"And\", \"But\", \"Miss\", \"'s\", \"It\", \"The\", \"He\", \"You\", \"/\", \")\", \"(\", \"\\n\", \"\", \"&amp\"]\n",
    "for x in range(len(otherTokens)):\n",
    "    stoplist.append(otherTokens[x])\n",
    "delimeters = \",./)(:;-\\n!\"\n",
    "for tweet in file:\n",
    "    for i in delimeters: \n",
    "        if i == \"\\n\" or i == \"-\":\n",
    "            tweet = tweet.replace(i , \" \")\n",
    "        else: \n",
    "            tweet = tweet.replace(i , \"\")\n",
    "    list_tweet_words= tweet.split(\" \")\n",
    "    for i in list_tweet_words:\n",
    "        if i in data_list:\n",
    "            num_list[data_list.index(i)]+=1\n",
    "        else:\n",
    "            if \"http\" not in i and i not in stoplist:\n",
    "                data_list.append(i)\n",
    "                num_list.append(1)\n",
    "repub_tweet_list = []\n",
    "for i in range(len(data_list)):\n",
    "    repub_tweet_list.append([data_list[i], num_list[i]])\n",
    "# print(repub_tweet_list)\n",
    "print(\"The length of the number of Republican indepent words is below and can see the words and their number of counts\")\n",
    "print(len(repub_tweet_list))\n",
    "max_repub_list = sorted(num_list)[-101:]\n",
    "# print(max_repub_list)\n",
    "percentage_repub = []\n",
    "repub_list_sum = sum(max_repub_list)\n",
    "# print(repub_list_sum)\n",
    "max_use_repub_list = []\n",
    "while len(max_repub_list) != 0:\n",
    "    for e in repub_tweet_list:\n",
    "        if e[1] == max_repub_list[-1]:\n",
    "            percentage_repub.append(round(max_repub_list[-1]/repub_list_sum,4))\n",
    "            max_use_repub_list.append(e[0])\n",
    "            max_repub_list = max_repub_list[:-1]\n",
    "            if len(max_repub_list) == 0:\n",
    "                break\n",
    "# print(max_use_repub_list)\n",
    "repub_max_words_weight_list = []\n",
    "for i in range(len(percentage_repub)):\n",
    "    repub_max_words_weight_list.append([percentage_repub[i],max_use_repub_list[i]])\n",
    "print(repub_max_words_weight_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_files/Hoyer_tweet_database.txt\n",
      "Probability for one tweet:\n",
      "Republican probability: 0.0577\n",
      "Democratic probability: 0.053700000000000005\n",
      "This tweet was more likely posted by a Republican\n",
      "Probability for 500 tweets:\n",
      "Republican probability: 0.7703000000000002\n",
      "Democratic probability: 0.8447\n",
      "This tweet was more likely posted by a Democrat\n",
      "Probability for 1000 tweets:\n",
      "Republican probability: 0.7703000000000002\n",
      "Democratic probability: 0.8447\n",
      "This tweet was more likely posted by a Democrat\n",
      "\n",
      "text_files/Buttigieg_tweet_database.txt\n",
      "Probability for one tweet:\n",
      "Republican probability: 0.0638\n",
      "Democratic probability: 0.0719\n",
      "This tweet was more likely posted by a Democrat\n",
      "Probability for 500 tweets:\n",
      "Republican probability: 0.6981999999999999\n",
      "Democratic probability: 0.9136\n",
      "This tweet was more likely posted by a Democrat\n",
      "Probability for 1000 tweets:\n",
      "Republican probability: 0.6981999999999999\n",
      "Democratic probability: 0.9136\n",
      "This tweet was more likely posted by a Democrat\n",
      "\n",
      "text_files/McCarthy_tweet_database.txt\n",
      "Probability for one tweet:\n",
      "Republican probability: 0.0788\n",
      "Democratic probability: 0.0693\n",
      "This tweet was more likely posted by a Republican\n",
      "Probability for 500 tweets:\n",
      "Republican probability: 0.6641999999999998\n",
      "Democratic probability: 0.6984\n",
      "This tweet was more likely posted by a Democrat\n",
      "Probability for 1000 tweets:\n",
      "Republican probability: 0.6641999999999998\n",
      "Democratic probability: 0.6984\n",
      "This tweet was more likely posted by a Democrat\n",
      "\n",
      "text_files/Cornyn_tweet_database.txt\n",
      "Probability for one tweet:\n",
      "Republican probability: 0\n",
      "Democratic probability: 0.0057\n",
      "This tweet was more likely posted by a Democrat\n",
      "Probability for 500 tweets:\n",
      "Republican probability: 0.6617999999999998\n",
      "Democratic probability: 0.6235000000000002\n",
      "This tweet was more likely posted by a Republican\n",
      "Probability for 1000 tweets:\n",
      "Republican probability: 0.6617999999999998\n",
      "Democratic probability: 0.6235000000000002\n",
      "This tweet was more likely posted by a Republican\n",
      "\n",
      "text_files/Booker_tweet_database.txt\n",
      "Probability for one tweet:\n",
      "Republican probability: 0.0543\n",
      "Democratic probability: 0.0778\n",
      "This tweet was more likely posted by a Democrat\n",
      "Probability for 500 tweets:\n",
      "Republican probability: 0.7008000000000001\n",
      "Democratic probability: 0.9135\n",
      "This tweet was more likely posted by a Democrat\n",
      "Probability for 1000 tweets:\n",
      "Republican probability: 0.7008000000000001\n",
      "Democratic probability: 0.9135\n",
      "This tweet was more likely posted by a Democrat\n",
      "\n",
      "text_files/Cortez_tweet_database.txt\n",
      "Probability for one tweet:\n",
      "Republican probability: 0.025500000000000002\n",
      "Democratic probability: 0.0216\n",
      "This tweet was more likely posted by a Republican\n",
      "Probability for 500 tweets:\n",
      "Republican probability: 0.6791999999999998\n",
      "Democratic probability: 0.8699\n",
      "This tweet was more likely posted by a Democrat\n",
      "Probability for 1000 tweets:\n",
      "Republican probability: 0.6791999999999998\n",
      "Democratic probability: 0.8699\n",
      "This tweet was more likely posted by a Democrat\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Buttigieg_list = []\n",
    "# McCarthy_list = []\n",
    "# Hoyer_list = []\n",
    "# Cornyn_list = []\n",
    "Booker_list = []\n",
    "Cortez_list = []\n",
    "Feinstein_list =[]\n",
    "Rubio_list = []\n",
    "Menendez_list = []\n",
    "Schumer_list = []\n",
    "McConnell_list = []\n",
    "Nunes_list = []\n",
    "Limbaugh_list = []\n",
    "Boozman_list = []\n",
    "Pence_list = []\n",
    "Burr_list = []\n",
    "Cruz_list = []\n",
    "Graham_list = []\n",
    "Crenshaw_list = []\n",
    "Ernst_list = []\n",
    "file_Booker = open(\"Booker_tweet_database.txt\", \"w\")\n",
    "file_Cortez = open(\"Cortez_tweet_database.txt\", \"w\")\n",
    "file_Feinstein = open(\"Feinstein_tweet_database.txt\", \"w\")\n",
    "file_Rubio = open(\"Rubio_tweet_database.txt\", \"w\")\n",
    "file_Menendez = open(\"Menendez_tweet_database.txt\", \"w\")\n",
    "file_Schumer = open(\"Schumer_tweet_database.txt\", \"w\")\n",
    "file_McConnell = open(\"McConnell_tweet_database.txt\", \"w\")\n",
    "file_Nunes = open(\"Nunes_tweet_database.txt\", \"w\")\n",
    "file_Limbaugh = open(\"Limbaugh_tweet_database.txt\", \"w\")\n",
    "file_Boozman = open(\"Boozman_tweet_database.txt\", \"w\")\n",
    "file_Pence = open(\"Pence_tweet_database.txt\", \"w\")\n",
    "file_Burr = open(\"Burr_tweet_database.txt\", \"w\")\n",
    "file_Cruz = open(\"Cruz_tweet_database.txt\", \"w\")\n",
    "file_Graham = open(\"Graham_tweet_database.txt\", \"w\")\n",
    "file_Crenshaw = open(\"Crenshaw_tweet_database.txt\", \"w\")\n",
    "file_Ernst = open(\"Ernst_tweet_database.txt\", \"w\")\n",
    "# file_B = open(\"Buttigieg_tweet_database.txt\", \"w\")  #IS A DEMOCRAT\n",
    "# file_M = open(\"McCarthy_tweet_database.txt\", \"w\")  #IS A REPUBLICAN\n",
    "# file_H = open(\"Hoyer_tweet_database.txt\", \"w\")  #IS A DEMOCRAT\n",
    "# file_C = open(\"Cornyn_tweet_database.txt\", \"w\") # IS A REPUBLICAN\n",
    "# Buttigieg_tweets = api.user_timeline(screen_name = \"PeteButtigieg\", count = 1000, include_rts = False, tweet_mode = \"extended\")\n",
    "# McCarthy_tweets = api.user_timeline(screen_name = \"GOPLeader\", count = 1000, include_rts = False, tweet_mode = \"extended\")\n",
    "# Hoyer_tweets = api.user_timeline(screen_name = \"LeaderHoyer\", count = 1000, include_rts = False, tweet_mode = \"extended\")\n",
    "# Cornyn_tweets = api.user_timeline(screen_name = \"JohnCornyn\", count = 1000, include_rts = False, tweet_mode = \"extended\")\n",
    "# Booker_tweets = api.user_timeline(screen_name = \"CoryBooker\", count = 1000, include_rts = False, tweet_mode = \"extended\")\n",
    "# Cortez_tweets = api.user_timeline(screen_name = \"AOC\", count = 1000, include_rts = False, tweet_mode = \"extended\")\n",
    "# Feinstein_tweets = api.user_timeline(screen_name = \"SenFeinstein\", count = 1000, include_rts = False, tweet_mode = \"extended\")\n",
    "# Rubio_tweets = api.user_timeline(screen_name = \"MarcoRubio\", count = 1000, include_rts = False, tweet_mode = \"extended\")\n",
    "# Menendez_tweets = api.user_timeline(screen_name = \"SenatorMenendez\", count = 1000, include_rts = False, tweet_mode = \"extended\")\n",
    "# Schumer_tweets = api.user_timeline(screen_name = \"SenSchumer\", count = 1000, include_rts = False, tweet_mode = \"extended\")\n",
    "# McConnell_tweets = api.user_timeline(screen_name = \"senatemajldr\", count = 1000, include_rts = False, tweet_mode = \"extended\")\n",
    "# Nunes_tweets = api.user_timeline(screen_name = \"DevinNunes\", count = 1000, include_rts = False, tweet_mode = \"extended\")\n",
    "# Limbaugh_tweets = api.user_timeline(screen_name = \"limbaugh\", count = 1000, include_rts = False, tweet_mode = \"extended\")\n",
    "# Boozman_tweets = api.user_timeline(screen_name = \"JohnBoozman\", count = 1000, include_rts = False, tweet_mode = \"extended\")\n",
    "# Pence_tweets = api.user_timeline(screen_name = \"VP\", count = 1000, include_rts = False, tweet_mode = \"extended\")\n",
    "# Burr_tweets = api.user_timeline(screen_name = \"SenatorBurr\", count = 1000, include_rts = False, tweet_mode = \"extended\")\n",
    "# Cruz_tweets = api.user_timeline(screen_name = \"tedcruz\", count = 1000, include_rts = False, tweet_mode = \"extended\")\n",
    "# Graham_tweets = api.user_timeline(screen_name = \"LindseyGrahamSC\", count = 1000, include_rts = False, tweet_mode = \"extended\")\n",
    "# Crenshaw_tweets = api.user_timeline(screen_name = \"RepDanCrenshaw\", count = 1000, include_rts = False, tweet_mode = \"extended\")\n",
    "# Ernst_tweets = api.user_timeline(screen_name = \"SenJoniErnst\", count = 1000, include_rts = False, tweet_mode = \"extended\")\n",
    "# for tweet in Buttigieg_tweets:\n",
    "#     Buttigieg_list.append(tweet.full_text)\n",
    "# for tweet in McCarthy_tweets:\n",
    "#     McCarthy_list.append(tweet.full_text)\n",
    "# for tweet in Hoyer_tweets:\n",
    "#     Hoyer_list.append(tweet.full_text)\n",
    "# for tweet in Cornyn_tweets:\n",
    "#     Cornyn_list.append(tweet.full_text)\n",
    "# for tweet in Booker_tweets:\n",
    "#     Booker_list.append(tweet.full_text)\n",
    "# for tweet in Cortez_tweets:\n",
    "#     Cortez_list.append(tweet.full_text)\n",
    "# for tweet in Feinstein_tweets:\n",
    "#     Feinstein_list.append(tweet.full_text)\n",
    "# for tweet in Rubio_tweets:\n",
    "#     Rubio_list.append(tweet.full_text)\n",
    "# for tweet in Menendez_tweets:\n",
    "#     Menendez_list.append(tweet.full_text)\n",
    "# for tweet in Schumer_tweets:\n",
    "#     Schumer_list.append(tweet.full_text)\n",
    "# for tweet in McConnell_tweets:\n",
    "#     McConnell_list.append(tweet.full_text)\n",
    "# for tweet in Nunes_tweets:\n",
    "#     Nunes_list.append(tweet.full_text)\n",
    "# for tweet in Limbaugh_tweets:\n",
    "#     Limbaugh_list.append(tweet.full_text)\n",
    "# for tweet in Boozman_tweets:\n",
    "#     Boozman_list.append(tweet.full_text)\n",
    "# for tweet in Pence_tweets:\n",
    "#     Pence_list.append(tweet.full_text)\n",
    "# for tweet in Burr_tweets:\n",
    "#     Burr_list.append(tweet.full_text)\n",
    "# for tweet in Cruz_tweets:\n",
    "#     Cruz_list.append(tweet.full_text)\n",
    "# for tweet in Graham_tweets:\n",
    "#     Graham_list.append(tweet.full_text)\n",
    "# for tweet in Crenshaw_tweets:\n",
    "#     Crenshaw_list.append(tweet.full_text)\n",
    "# for tweet in Ernst_tweets:\n",
    "#     Ernst_list.append(tweet.full_text)\n",
    "                        \n",
    "    \n",
    "# for tweet in Buttigieg_list:\n",
    "#     file_B.write(tweet)\n",
    "#     file_B.write(\"\\n\")\n",
    "# for tweet in McCarthy_list:\n",
    "#     file_M.write(tweet)\n",
    "#     file_M.write(\"\\n\")\n",
    "# for tweet in Hoyer_list:\n",
    "#     file_H.write(tweet)\n",
    "#     file_H.write(\"\\n\")\n",
    "# for tweet in Cornyn_list:\n",
    "#     file_C.write(tweet)\n",
    "#     file_C.write(\"\\n\")\n",
    "# for tweet in Booker_list:\n",
    "#     file_Booker.write(tweet)\n",
    "#     file_Booker.write(\"\\n\")\n",
    "# file_Booker.close()\n",
    "# for tweet in Cortez_list:\n",
    "#     file_Cortez.write(tweet)\n",
    "#     file_Cortez.write(\"\\n\")\n",
    "# file_Cortez.close()\n",
    "# for tweet in Feinstein_list:\n",
    "#     file_Feinstein.write(tweet)\n",
    "#     file_Feinstein.write(\"\\n\")\n",
    "# file_Feinstein.close()\n",
    "# for tweet in Rubio_list:\n",
    "#     file_Rubio.write(tweet)\n",
    "#     file_Rubio.write(\"\\n\")\n",
    "# file_Rubio.close()\n",
    "# for tweet in Menendez_list:\n",
    "#     file_Menendez.write(tweet)\n",
    "#     file_Menendez.write(\"\\n\")\n",
    "# file_Menendez.close()\n",
    "# for tweet in Schumer_list:\n",
    "#     file_Schumer.write(tweet)\n",
    "#     file_Schumer.write(\"\\n\")\n",
    "# file_Schumer.close()\n",
    "# for tweet in McConnell_list:\n",
    "#     file_McConnell.write(tweet)\n",
    "#     file_McConnell.write(\"\\n\")\n",
    "# file_McConnell.close()\n",
    "# for tweet in Nunes_list:\n",
    "#     file_Nunes.write(tweet)\n",
    "#     file_Nunes.write(\"\\n\")\n",
    "# file_Nunes.close()\n",
    "# for tweet in Limbaugh_list:\n",
    "#     file_Limbaugh.write(tweet)\n",
    "#     file_Limbaugh.write(\"\\n\")\n",
    "# file_Limbaugh.close()\n",
    "# for tweet in Boozman_list:\n",
    "#     file_Boozman.write(tweet)\n",
    "#     file_Boozman.write(\"\\n\")\n",
    "# file_Boozman.close()\n",
    "# for tweet in Pence_list:\n",
    "#     file_Pence.write(tweet)\n",
    "#     file_Pence.write(\"\\n\")\n",
    "# file_Pence.close()\n",
    "# for tweet in Burr_list:\n",
    "#     file_Burr.write(tweet)\n",
    "#     file_Burr.write(\"\\n\")\n",
    "# file_Burr.close()\n",
    "# for tweet in Cruz_list:\n",
    "#     file_Cruz.write(tweet)\n",
    "#     file_Cruz.write(\"\\n\")\n",
    "# file_Cruz.close()\n",
    "# for tweet in Graham_list:\n",
    "#     file_Graham.write(tweet)\n",
    "#     file_Graham.write(\"\\n\")\n",
    "# file_Graham.close()\n",
    "# for tweet in Crenshaw_list:\n",
    "#     file_Crenshaw.write(tweet)\n",
    "#     file_Crenshaw.write(\"\\n\")\n",
    "# file_Crenshaw.close()\n",
    "# for tweet in Ernst_list:\n",
    "#     file_Ernst.write(tweet)\n",
    "#     file_Ernst.write(\"\\n\")\n",
    "# file_Ernst.close()\n",
    "# file_B.close()\n",
    "# file_M.close()\n",
    "# file_H.close()\n",
    "# file_C.close()\n",
    "\n",
    "Hoyer = \"text_files/Hoyer_tweet_database.txt\"\n",
    "Buttigieg = \"text_files/Buttigieg_tweet_database.txt\"\n",
    "McCarthy = \"text_files/McCarthy_tweet_database.txt\"\n",
    "Cornyn = \"text_files/Cornyn_tweet_database.txt\"\n",
    "Booker = \"text_files/Booker_tweet_database.txt\"\n",
    "Cortez = \"text_files/Cortez_tweet_database.txt\"\n",
    "Rubio = \"text_files/Rubio_tweet_database.txt\"\n",
    "Menendez = \"text_files/Menendez_tweet_database.txt\"\n",
    "Schumer = \"text_files/Schumer_tweet_database.txt\"\n",
    "McConnell = \"text_files/McConnell_tweet_database.txt\"\n",
    "Nunes = \"text_files/Nunes_tweet_database.txt\"\n",
    "Limbaugh = \"text_files/Limbaugh_tweet_database.txt\"\n",
    "Boozman = \"text_files/Boozman_tweet_database.txt\"\n",
    "Pence = \"text_files/Pence_tweet_database.txt\"\n",
    "Burr = \"text_files/Burr_tweet_database.txt\"\n",
    "Cruz = \"text_files/Cruz_tweet_database.txt\"\n",
    "Graham = \"text_files/Graham_tweet_database.txt\"\n",
    "Crenshaw = \"text_files/Crenshaw_tweet_database.txt\"\n",
    "Ernst = \"text_files/Ernst_tweet_database.txt\"\n",
    "people = []\n",
    "people.append(Hoyer)\n",
    "people.append(Buttigieg)\n",
    "people.append(McCarthy)\n",
    "people.append(Cornyn)\n",
    "people.append(Booker)\n",
    "people.append(Cortez)\n",
    "# people.append(Rubio)\n",
    "# people.append(Menendez)\n",
    "# people.append(Schumer)\n",
    "# people.append(McConnell)\n",
    "# people.append(Nunes)\n",
    "# people.append(Limbaugh)\n",
    "# people.append(Boozman)\n",
    "# people.append(Pence)\n",
    "# people.append(Burr)\n",
    "# people.append(Cruz)\n",
    "# people.append(Graham)\n",
    "# people.append(Crenshaw)\n",
    "# people.append(Ernst)\n",
    "for element in people:\n",
    "    test_list_one = []\n",
    "    test_list_five_hundred = []\n",
    "    test_list_thousand = []\n",
    "    testing_list_one = []\n",
    "    testing_list_five_hundred = []\n",
    "    testing_list_thousand = []\n",
    "    file = open(element, \"r\")\n",
    "#     if element[11] == \"H\":\n",
    "#         print(\"Hoyer is a Democrat, we are using sample labeling to test out our word frequency classifier.\")\n",
    "#     elif element[11] == \"B\":\n",
    "#         print(\"Buttigieg is a Democrat, we are using sample labeling to test out our word frequency classifier.\")\n",
    "#     elif element[11] == \"M\":\n",
    "#         print(\"McCarthy is a Republican, we are using sample labeling to test out our word frequency classifier.\")\n",
    "#     else:\n",
    "        #print(\"Cornyn is a Republican, we are using sample labeling to test out our word frequency classifier.\")\n",
    "    print(element)\n",
    "    lines = file.readlines()\n",
    "    count = 0\n",
    "    for line in lines:\n",
    "#         print(line)\n",
    "        if count >= 1:\n",
    "            if count >= 500:\n",
    "                test_list_thousand.append(line)\n",
    "            else:\n",
    "                test_list_five_hundred.append(line)\n",
    "                test_list_thousand.append(line)\n",
    "        else:\n",
    "            test_list_one.append(line)\n",
    "            test_list_thousand.append(line)\n",
    "            test_list_five_hundred.append(line)\n",
    "        count += 1\n",
    "#         print(count)\n",
    "\n",
    "    stoplist = stopwords.words('english')\n",
    "    delimeters = \",./)(:;\\n-!\"\n",
    "    otherTokens= [\"yes\", \"okay\", \"ok\", \",\", \";\", \":\", \".\", \"?\", '”', \"--\", \"!\", \"I\", \"n't\", \"would\", \"could\", '“',\n",
    "                 \"Dr.\", \"Mrs.\", \"And\", \"But\", \"Miss\", \"'s\", \"It\", \"The\", \"He\", \"You\", \"/\", \")\", \"(\", \"\\n\", \"\", \"&amp\"]\n",
    "    for x in range(len(otherTokens)):\n",
    "        stoplist.append(otherTokens[x])\n",
    "\n",
    "    for tweet in test_list_one:\n",
    "        for i in delimeters: \n",
    "            if i ==\"\\n\" or i == \"-\":\n",
    "                tweet = tweet.replace(i , \" \")\n",
    "            else: \n",
    "                tweet = tweet.replace(i , \"\")\n",
    "        list_tweet_words= tweet.split(\" \")\n",
    "        for i in list_tweet_words:\n",
    "            if \"http\" not in i and i not in stoplist and i not in testing_list_one:\n",
    "                testing_list_one.append(i)\n",
    "\n",
    "    for tweet in test_list_five_hundred:\n",
    "        for i in delimeters: \n",
    "            if i ==\"\\n\" or i == \"-\":\n",
    "                tweet = tweet.replace(i , \" \")\n",
    "            else: \n",
    "                tweet = tweet.replace(i , \"\")\n",
    "        list_tweet_words= tweet.split(\" \")\n",
    "        for i in list_tweet_words:\n",
    "            if \"http\" not in i and i not in stoplist and i not in testing_list_five_hundred:\n",
    "                testing_list_five_hundred.append(i) \n",
    "\n",
    "    for tweet in test_list_thousand:\n",
    "        for i in delimeters: \n",
    "            if i ==\"\\n\" or i == \"-\":\n",
    "                tweet = tweet.replace(i , \" \")\n",
    "            else: \n",
    "                tweet = tweet.replace(i , \"\")\n",
    "        list_tweet_words= tweet.split(\" \")\n",
    "        for i in list_tweet_words:\n",
    "            if \"http\" not in i and i not in stoplist and i not in testing_list_thousand:\n",
    "                testing_list_thousand.append(i) \n",
    "\n",
    "    demo_prob = 0\n",
    "    repub_prob = 0\n",
    "    print(\"Probability for one tweet:\")\n",
    "    for element in testing_list_one:\n",
    "        for word in repub_max_words_weight_list: \n",
    "            if element == word[1]:\n",
    "                repub_prob += word[0]\n",
    "        for word in demo_max_words_weight_list: \n",
    "            if element == word[1]:\n",
    "                demo_prob += word[0]\n",
    "    print(\"Republican probability:\", repub_prob)\n",
    "    print(\"Democratic probability:\", demo_prob)\n",
    "    if demo_prob > repub_prob:\n",
    "        print(\"This tweet was more likely posted by a Democrat\")\n",
    "    else:\n",
    "        print(\"This tweet was more likely posted by a Republican\")\n",
    "\n",
    "    demo_prob = 0\n",
    "    repub_prob = 0\n",
    "    print(\"Probability for 500 tweets:\")\n",
    "    for element in testing_list_five_hundred:\n",
    "        for word in repub_max_words_weight_list: \n",
    "            if element == word[1]:\n",
    "                repub_prob += word[0]\n",
    "        for word in demo_max_words_weight_list: \n",
    "            if element == word[1]:\n",
    "                demo_prob += word[0]\n",
    "    print(\"Republican probability:\", repub_prob)\n",
    "    print(\"Democratic probability:\", demo_prob)\n",
    "    if demo_prob > repub_prob:\n",
    "        print(\"This tweet was more likely posted by a Democrat\")\n",
    "    else:\n",
    "        print(\"This tweet was more likely posted by a Republican\")\n",
    "\n",
    "    demo_prob = 0\n",
    "    repub_prob = 0\n",
    "    print(\"Probability for 1000 tweets:\")\n",
    "    for element in testing_list_thousand:\n",
    "        for word in repub_max_words_weight_list: \n",
    "            if element == word[1]:\n",
    "                repub_prob += word[0]\n",
    "        for word in demo_max_words_weight_list: \n",
    "            if element == word[1]:\n",
    "                demo_prob += word[0]\n",
    "    print(\"Republican probability:\", repub_prob)\n",
    "    print(\"Democratic probability:\", demo_prob)\n",
    "    if demo_prob > repub_prob:\n",
    "        print(\"This tweet was more likely posted by a Democrat\")\n",
    "    else:\n",
    "        print(\"This tweet was more likely posted by a Republican\")\n",
    "    print(\"\")\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Republican Max Use Word Cloud\n"
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import requests\n",
    "dataset = \"\"\n",
    "for line in max_use_repub_list:\n",
    "    dataset += line.lower()\n",
    "    dataset += \" \"\n",
    "mask = np.array(Image.open(requests.get(\"http://www.clker.com/cliparts/O/i/x/Y/q/P/yellow-house-hi.png\", stream = True).raw))\n",
    "def generate_wordcloud(words, mask):\n",
    "    word_cloud = WordCloud(width = 512, height = 512, background_color='white', stopwords=STOPWORDS, mask=mask).generate(words)\n",
    "    word_cloud.to_file(\"repub_wordCloud.png\")\n",
    "print(\"Creating Republican Max Use Word Cloud\")\n",
    "generate_wordcloud(dataset, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Democrat Max Use Word Cloud\n"
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import requests\n",
    "dataset = \"\"\n",
    "for line in max_use_demo_list:\n",
    "    dataset += line.lower()\n",
    "    dataset += \" \"\n",
    "mask = np.array(Image.open(requests.get(\"http://www.clker.com/cliparts/O/i/x/Y/q/P/yellow-house-hi.png\", stream = True).raw))\n",
    "def generate_wordcloud(words, mask):\n",
    "    word_cloud = WordCloud(width = 512, height = 512, background_color='white', stopwords=STOPWORDS, mask=mask).generate(words)\n",
    "    word_cloud.to_file(\"demo_wordCloud.png\")\n",
    "print(\"Creating Democrat Max Use Word Cloud\")\n",
    "generate_wordcloud(dataset, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mpeople              \u001b[34mWe\n",
      "\u001b[31mUS                  \u001b[34mpeople\n",
      "\u001b[31mThis                \u001b[34mus\n",
      "\u001b[31mTrump               \u001b[34mcountry\n",
      "\u001b[31mUtah                \u001b[34mfight\n",
      "\u001b[31mgreat               \u001b[34mTrump\n",
      "\u001b[31mWe                  \u001b[34mmake\n",
      "\u001b[31mThank               \u001b[34mone\n",
      "\u001b[31mHouse               \u001b[34mI’m\n",
      "\u001b[31mImpeachment         \u001b[34mneed\n",
      "\u001b[31mwork                \u001b[34mmust\n",
      "\u001b[31mwant                \u001b[34mpresident\n",
      "\u001b[31m#openbordersinc     \u001b[34mwork\n",
      "\u001b[31mPresident           \u001b[34m#DemDebate\n",
      "\u001b[31mcountry             \u001b[34mAmerican\n",
      "\u001b[31mtoday               \u001b[34mAmericans\n",
      "\u001b[31mmust                \u001b[34mtime\n",
      "\u001b[31mone                 \u001b[34mtake\n",
      "\u001b[31mMy                  \u001b[34mAmerica\n",
      "\u001b[31mAmerican            \u001b[34mstand\n",
      "\u001b[31mI’m                 \u001b[34mcare\n",
      "\u001b[31m@RobSchneider       \u001b[34mThis\n",
      "\u001b[31mvaping              \u001b[34m—\n",
      "\u001b[31myears               \u001b[34mplan\n",
      "\u001b[31mSchiff              \u001b[34mworld\n",
      "\u001b[31mDemocrats           \u001b[34mHouse\n",
      "\u001b[31mpublic              \u001b[34mhealth\n",
      "\u001b[31m@dcexaminer         \u001b[34mlike\n",
      "\u001b[31mus                  \u001b[34mPresident\n",
      "\u001b[31mkeep                \u001b[34mevery\n",
      "\u001b[31mstate               \u001b[34mchange\n",
      "\u001b[31mlike                \u001b[34mrights\n",
      "\u001b[31myoung               \u001b[34mget\n",
      "\u001b[31mknow                \u001b[34mgoing\n",
      "\u001b[31mback                \u001b[34mfamilies\n",
      "\u001b[31mright               \u001b[34mwomen\n",
      "\u001b[31mnew                 \u001b[34mright\n",
      "\u001b[31mlast                \u001b[34mDonald\n",
      "\u001b[31mhearing             \u001b[34m@AyannaPressley\n",
      "\u001b[31mneed                \u001b[34mday\n",
      "\u001b[31mA                   \u001b[34mtogether\n",
      "\u001b[31mSyria               \u001b[34mend\n",
      "\u001b[31mAmericans           \u001b[34mpower\n",
      "\u001b[31mThanks              \u001b[34mfirst\n",
      "\u001b[31mget                 \u001b[34mproud\n",
      "\u001b[31mday                 \u001b[34mprotect\n",
      "\u001b[31mAs                  \u001b[34mIf\n",
      "\u001b[31mfirst               \u001b[34mWhen\n",
      "\u001b[31mfriend              \u001b[34mworkers\n",
      "\u001b[31mdebt                \u001b[34mIn\n",
      "\u001b[31msecurity            \u001b[34mviolence\n",
      "\u001b[31mIran                \u001b[34mAs\n",
      "\u001b[31mservice             \u001b[34mvote\n",
      "\u001b[31mAmerica             \u001b[34mI'm\n",
      "\u001b[31mforward             \u001b[34mfighting\n",
      "\u001b[31mcolleagues          \u001b[34mfuture\n",
      "\u001b[31mcrisis              \u001b[34mtoday\n",
      "\u001b[31mUtahns              \u001b[34myears\n",
      "\u001b[31mmany                \u001b[34mwant\n",
      "\u001b[31mfuture              \u001b[34mhelp\n",
      "\u001b[31mdecision            \u001b[34myoung\n",
      "\u001b[31mIn                  \u001b[34mhistory\n",
      "\u001b[31mtown                \u001b[34macross\n",
      "\u001b[31mgo                  \u001b[34mToday\n",
      "\u001b[31mbest                \u001b[34mcampaign\n",
      "\u001b[31mprotect             \u001b[34mBlack\n",
      "\u001b[31msupport             \u001b[34mgreat\n",
      "\u001b[31mnational            \u001b[34myear\n",
      "\u001b[31mworking             \u001b[34mworking\n",
      "\u001b[31mtime                \u001b[34msupport\n",
      "\u001b[31mimportant           \u001b[34mbetter\n",
      "\u001b[31mUtah’s              \u001b[34mfamily\n",
      "\u001b[31mwell                \u001b[34mlive\n",
      "\u001b[31mhelp                \u001b[34mWhite\n",
      "\u001b[31mIt’s                \u001b[34mwhite\n",
      "\u001b[31mworkers             \u001b[34mensure\n",
      "\u001b[31m1                   \u001b[34mOur\n",
      "\u001b[31mgood                \u001b[34mIt’s\n",
      "\u001b[31mthing               \u001b[34mlaw\n",
      "\u001b[31mGreat               \u001b[34mbig\n",
      "\u001b[31mNo                  \u001b[34mbuild\n",
      "\u001b[31m#AmericaFirst       \u001b[34mwin\n",
      "\u001b[31mSpeaker             \u001b[34mclimate\n",
      "\u001b[31mBiden               \u001b[34mUS\n",
      "\u001b[31mCongress            \u001b[34mnation\n",
      "\u001b[31mhealth              \u001b[34mknow\n",
      "\u001b[31mnews                \u001b[34munion\n",
      "\u001b[31maddress             \u001b[34mgot\n",
      "\u001b[31mgoing               \u001b[34mjustice\n",
      "\u001b[31mfamily              \u001b[34mjoin\n",
      "\u001b[31mmuch                \u001b[34mAll\n",
      "\u001b[31mallies              \u001b[34mleaders\n",
      "\u001b[31myear                \u001b[34malways\n",
      "\u001b[31mhigh                \u001b[34mcommunities\n",
      "\u001b[31mnight               \u001b[34mway\n",
      "\u001b[31m@Apple              \u001b[34mit’s\n",
      "\u001b[31m@sanctuary_no       \u001b[34msee\n",
      "\u001b[31m@JessicaV_CIS       \u001b[34mclass\n",
      "\u001b[31mBoston              \u001b[34mpublic\n",
      "\u001b[31mimpeachment         \u001b[34mAct\n",
      "\u001b[31mUkraine             \u001b[34mlife\n"
     ]
    }
   ],
   "source": [
    "from colorama import Fore\n",
    "for i in range(len(max_use_demo_list)):\n",
    "    string = \"\"\n",
    "    string += Fore.RED + max_use_repub_list[i]\n",
    "    string += (20-len(max_use_repub_list[i])) * \" \"\n",
    "    string += Fore.BLUE + max_use_demo_list[i]\n",
    "    print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bag of Words Code Here\n",
    "# I HAVE NO IDEA WHERE TO START SO NEED SOME TIME WHEN STARTING THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance of Democrat's sentiment: 0.05214055759128901\n",
      "Variance of Republican's sentiment: 0.06434247403905909\n",
      "Variance of Democrat's subjectivity: 0.0860721283450884\n",
      "Variance of Republican's subjectivity: 0.08971695773282033\n",
      "Republican and Democratic sentiment statistical difference.\n",
      "t = 5.26276097110456\n",
      "p = 1.4276183926145473e-07\n",
      "Republican and Democratic subjectivity statistical difference.\n",
      "t = 5.674436194826788\n",
      "p = 1.4026832193812037e-08\n"
     ]
    }
   ],
   "source": [
    "## Import the packages\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "file_R = open(\"text_files/republican_tweet_database.txt\",\"r\")\n",
    "repub_senti_list = []\n",
    "repub_subject_list = []\n",
    "count = 0\n",
    "for tweet in file_R:\n",
    "    tweeter = TextBlob(str(tweet))\n",
    "    tweet_senti = tweeter.sentiment.polarity\n",
    "    tweet_subject = tweeter.sentiment.subjectivity\n",
    "    repub_senti_list.append(tweet_senti)\n",
    "    repub_subject_list.append(tweet_subject)\n",
    "    count += 1\n",
    "file_R.close()\n",
    "\n",
    "file_D = open(\"text_files/democrat_tweet_database.txt\",\"r\")\n",
    "demo_senti_list = []\n",
    "demo_subject_list = []\n",
    "count = 0\n",
    "for tweet in file_D:\n",
    "    tweeter = TextBlob(str(tweet))\n",
    "    tweet_senti = tweeter.sentiment.polarity\n",
    "    tweet_subject = tweeter.sentiment.subjectivity\n",
    "    demo_senti_list.append(tweet_senti)\n",
    "    demo_subject_list.append(tweet_subject)\n",
    "    count += 1\n",
    "file_D.close()\n",
    "\n",
    "a = np.array(demo_senti_list)\n",
    "b = np.array(repub_senti_list)\n",
    "c = np.array(demo_subject_list)\n",
    "d = np.array(repub_subject_list)\n",
    "#For unbiased max likelihood estimate we have to divide the var by N-1, and therefore the parameter ddof = 1\n",
    "var_a = a.var(ddof = 1)\n",
    "print(\"Variance of Democrat's sentiment:\", var_a)\n",
    "var_b = b.var(ddof = 1)\n",
    "print(\"Variance of Republican's sentiment:\", var_b)\n",
    "var_c = c.var(ddof = 1)\n",
    "print(\"Variance of Democrat's subjectivity:\", var_c)\n",
    "var_d = d.var(ddof = 1)\n",
    "print(\"Variance of Republican's subjectivity:\", var_d)\n",
    "#std deviation\n",
    "s = np.sqrt((var_a + var_b)/2)\n",
    "s2 = np.sqrt((var_c + var_d)/2)\n",
    "## Calculate the t-statistics\n",
    "t = (a.mean() - b.mean())/(s*np.sqrt(2/count))\n",
    "t2 = (c.mean() - d.mean())/(s*np.sqrt(2/count))\n",
    "## Compare with the critical t-value\n",
    "#Degrees of freedom\n",
    "df = 2*count - 2\n",
    "df2 = 2*count - 2\n",
    "#p-value after comparison with the t \n",
    "# p = 1 - stats.t.cdf(t,df=df)\n",
    "# print(\"t = \" + str(t))\n",
    "# print(\"p = \" + str(2*p))\n",
    "## Cross Checking with the internal scipy function\n",
    "print(\"Republican and Democratic sentiment statistical difference.\")\n",
    "t, p = stats.ttest_ind(a,b)\n",
    "print(\"t = \" + str(t))\n",
    "print(\"p = \" + str(p))\n",
    "print(\"Republican and Democratic subjectivity statistical difference.\")\n",
    "t2, p2 = stats.ttest_ind(c,d)\n",
    "print(\"t = \" + str(t2))\n",
    "print(\"p = \" + str(p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hoyer is a Democrat, we are using sample labeling to test out our word frequency classifier.\n",
      "Probability:\n",
      "Republican probability: [[1, 'people'], [1, 'US'], [1, 'This'], [1, 'Trump'], [0, 'Utah'], [1, 'great'], [1, 'We'], [1, 'Thank'], [1, 'House'], [0, 'Impeachment'], [1, 'work'], [1, 'want'], [0, '#openbordersinc'], [1, 'President'], [1, 'country'], [1, 'today'], [1, 'must'], [1, 'one'], [1, 'My'], [1, 'American'], [1, 'I’m'], [0, '@RobSchneider'], [0, 'vaping'], [1, 'years'], [0, 'Schiff'], [1, 'Democrats'], [1, 'public'], [0, '@dcexaminer'], [1, 'us'], [1, 'keep'], [1, 'state'], [1, 'like'], [0, 'young'], [1, 'know'], [1, 'back'], [1, 'right'], [1, 'new'], [1, 'last'], [1, 'hearing'], [1, 'need'], [1, 'A'], [0, 'Syria'], [1, 'Americans'], [1, 'Thanks'], [1, 'get'], [1, 'day'], [1, 'As'], [1, 'first'], [1, 'friend'], [0, 'debt'], [1, 'security'], [0, 'Iran'], [1, 'service'], [0, 'America'], [1, 'forward'], [1, 'colleagues'], [1, 'crisis'], [0, 'Utahns'], [1, 'many'], [1, 'future'], [0, 'decision'], [1, 'In'], [0, 'town'], [1, 'go'], [1, 'best'], [1, 'protect'], [1, 'support'], [1, 'national'], [1, 'working'], [1, 'time'], [1, 'important'], [0, 'Utah’s'], [1, 'well'], [1, 'help'], [1, 'It’s'], [1, 'workers'], [0, '1'], [1, 'good'], [1, 'thing'], [1, 'Great'], [1, 'No'], [0, '#AmericaFirst'], [0, 'Speaker'], [1, 'Biden'], [1, 'Congress'], [1, 'health'], [1, 'news'], [1, 'address'], [0, 'going'], [1, 'family'], [1, 'much'], [1, 'allies'], [1, 'year'], [0, 'high'], [1, 'night'], [0, '@Apple'], [0, '@sanctuary_no'], [0, '@JessicaV_CIS'], [0, 'Boston'], [1, 'impeachment'], [1, 'Ukraine']]\n",
      "Democratic probability: [[1, 'We'], [1, 'people'], [1, 'us'], [1, 'country'], [1, 'fight'], [1, 'Trump'], [1, 'make'], [1, 'one'], [1, 'I’m'], [1, 'need'], [1, 'must'], [1, 'president'], [1, 'work'], [0, '#DemDebate'], [1, 'American'], [1, 'Americans'], [1, 'time'], [1, 'take'], [0, 'America'], [1, 'stand'], [1, 'care'], [1, 'This'], [0, '—'], [1, 'plan'], [1, 'world'], [1, 'House'], [1, 'health'], [1, 'like'], [1, 'President'], [1, 'every'], [1, 'change'], [1, 'rights'], [1, 'get'], [0, 'going'], [1, 'families'], [1, 'women'], [1, 'right'], [0, 'Donald'], [0, '@AyannaPressley'], [1, 'day'], [1, 'together'], [1, 'end'], [1, 'power'], [1, 'first'], [1, 'proud'], [1, 'protect'], [1, 'If'], [1, 'When'], [1, 'workers'], [1, 'In'], [1, 'violence'], [1, 'As'], [1, 'vote'], [0, \"I'm\"], [1, 'fighting'], [1, 'future'], [1, 'today'], [1, 'years'], [1, 'want'], [1, 'help'], [0, 'young'], [0, 'history'], [1, 'across'], [1, 'Today'], [1, 'campaign'], [0, 'Black'], [1, 'great'], [1, 'year'], [1, 'working'], [1, 'support'], [0, 'better'], [1, 'family'], [1, 'live'], [1, 'White'], [1, 'white'], [1, 'ensure'], [1, 'Our'], [1, 'It’s'], [1, 'law'], [0, 'big'], [1, 'build'], [0, 'win'], [1, 'climate'], [1, 'US'], [1, 'nation'], [1, 'know'], [0, 'union'], [0, 'got'], [1, 'justice'], [1, 'join'], [1, 'All'], [0, 'leaders'], [0, 'always'], [1, 'communities'], [1, 'way'], [1, 'it’s'], [1, 'see'], [0, 'class'], [1, 'public'], [1, 'Act'], [0, 'life']]\n",
      "Buttigieg is a Democrat, we are using sample labeling to test out our word frequency classifier.\n",
      "Probability:\n",
      "Republican probability: [[1, 'people'], [0, 'US'], [1, 'This'], [1, 'Trump'], [0, 'Utah'], [1, 'great'], [1, 'We'], [1, 'Thank'], [0, 'House'], [0, 'Impeachment'], [1, 'work'], [0, 'want'], [0, '#openbordersinc'], [1, 'President'], [1, 'country'], [1, 'today'], [1, 'must'], [1, 'one'], [1, 'My'], [1, 'American'], [1, 'I’m'], [0, '@RobSchneider'], [0, 'vaping'], [1, 'years'], [0, 'Schiff'], [1, 'Democrats'], [1, 'public'], [0, '@dcexaminer'], [1, 'us'], [1, 'keep'], [1, 'state'], [1, 'like'], [1, 'young'], [1, 'know'], [1, 'back'], [1, 'right'], [1, 'new'], [1, 'last'], [1, 'hearing'], [1, 'need'], [1, 'A'], [0, 'Syria'], [1, 'Americans'], [1, 'Thanks'], [1, 'get'], [1, 'day'], [1, 'As'], [1, 'first'], [0, 'friend'], [1, 'debt'], [1, 'security'], [0, 'Iran'], [1, 'service'], [1, 'America'], [1, 'forward'], [0, 'colleagues'], [1, 'crisis'], [0, 'Utahns'], [1, 'many'], [1, 'future'], [0, 'decision'], [1, 'In'], [1, 'town'], [1, 'go'], [1, 'best'], [1, 'protect'], [1, 'support'], [1, 'national'], [1, 'working'], [1, 'time'], [1, 'important'], [0, 'Utah’s'], [0, 'well'], [1, 'help'], [1, 'It’s'], [1, 'workers'], [0, '1'], [1, 'good'], [1, 'thing'], [1, 'Great'], [0, 'No'], [0, '#AmericaFirst'], [0, 'Speaker'], [0, 'Biden'], [1, 'Congress'], [1, 'health'], [1, 'news'], [0, 'address'], [1, 'going'], [1, 'family'], [0, 'much'], [1, 'allies'], [1, 'year'], [0, 'high'], [1, 'night'], [0, '@Apple'], [0, '@sanctuary_no'], [0, '@JessicaV_CIS'], [0, 'Boston'], [1, 'impeachment'], [0, 'Ukraine']]\n",
      "Democratic probability: [[1, 'We'], [1, 'people'], [1, 'us'], [1, 'country'], [1, 'fight'], [1, 'Trump'], [1, 'make'], [1, 'one'], [1, 'I’m'], [1, 'need'], [1, 'must'], [1, 'president'], [1, 'work'], [1, '#DemDebate'], [1, 'American'], [1, 'Americans'], [1, 'time'], [1, 'take'], [1, 'America'], [1, 'stand'], [1, 'care'], [1, 'This'], [0, '—'], [1, 'plan'], [1, 'world'], [0, 'House'], [1, 'health'], [1, 'like'], [1, 'President'], [1, 'every'], [1, 'change'], [1, 'rights'], [1, 'get'], [1, 'going'], [1, 'families'], [1, 'women'], [1, 'right'], [1, 'Donald'], [0, '@AyannaPressley'], [1, 'day'], [1, 'together'], [1, 'end'], [1, 'power'], [1, 'first'], [1, 'proud'], [1, 'protect'], [1, 'If'], [1, 'When'], [1, 'workers'], [1, 'In'], [1, 'violence'], [1, 'As'], [1, 'vote'], [1, \"I'm\"], [1, 'fighting'], [1, 'future'], [1, 'today'], [1, 'years'], [0, 'want'], [1, 'help'], [1, 'young'], [1, 'history'], [1, 'across'], [1, 'Today'], [1, 'campaign'], [1, 'Black'], [1, 'great'], [1, 'year'], [1, 'working'], [1, 'support'], [1, 'better'], [1, 'family'], [1, 'live'], [0, 'White'], [0, 'white'], [1, 'ensure'], [1, 'Our'], [1, 'It’s'], [1, 'law'], [0, 'big'], [1, 'build'], [1, 'win'], [1, 'climate'], [0, 'US'], [1, 'nation'], [1, 'know'], [0, 'union'], [1, 'got'], [1, 'justice'], [1, 'join'], [1, 'All'], [1, 'leaders'], [1, 'always'], [1, 'communities'], [1, 'way'], [1, 'it’s'], [0, 'see'], [1, 'class'], [1, 'public'], [0, 'Act'], [1, 'life']]\n",
      "McCarthy is a Republican, we are using sample labeling to test out our word frequency classifier.\n",
      "Probability:\n",
      "Republican probability: [[1, 'people'], [1, 'US'], [1, 'This'], [1, 'Trump'], [0, 'Utah'], [0, 'great'], [1, 'We'], [0, 'Thank'], [1, 'House'], [1, 'Impeachment'], [1, 'work'], [1, 'want'], [0, '#openbordersinc'], [1, 'President'], [1, 'country'], [1, 'today'], [0, 'must'], [1, 'one'], [0, 'My'], [1, 'American'], [1, 'I’m'], [0, '@RobSchneider'], [0, 'vaping'], [1, 'years'], [1, 'Schiff'], [1, 'Democrats'], [1, 'public'], [0, '@dcexaminer'], [1, 'us'], [1, 'keep'], [0, 'state'], [1, 'like'], [1, 'young'], [1, 'know'], [1, 'back'], [1, 'right'], [1, 'new'], [1, 'last'], [1, 'hearing'], [0, 'need'], [1, 'A'], [0, 'Syria'], [1, 'Americans'], [0, 'Thanks'], [1, 'get'], [1, 'day'], [0, 'As'], [1, 'first'], [1, 'friend'], [0, 'debt'], [1, 'security'], [1, 'Iran'], [0, 'service'], [1, 'America'], [0, 'forward'], [0, 'colleagues'], [0, 'crisis'], [0, 'Utahns'], [1, 'many'], [0, 'future'], [1, 'decision'], [1, 'In'], [1, 'town'], [1, 'go'], [1, 'best'], [0, 'protect'], [1, 'support'], [0, 'national'], [1, 'working'], [1, 'time'], [1, 'important'], [0, 'Utah’s'], [1, 'well'], [1, 'help'], [1, 'It’s'], [0, 'workers'], [1, '1'], [1, 'good'], [1, 'thing'], [0, 'Great'], [1, 'No'], [0, '#AmericaFirst'], [1, 'Speaker'], [0, 'Biden'], [1, 'Congress'], [1, 'health'], [1, 'news'], [0, 'address'], [1, 'going'], [1, 'family'], [1, 'much'], [0, 'allies'], [1, 'year'], [0, 'high'], [0, 'night'], [0, '@Apple'], [0, '@sanctuary_no'], [0, '@JessicaV_CIS'], [0, 'Boston'], [1, 'impeachment'], [1, 'Ukraine']]\n",
      "Democratic probability: [[1, 'We'], [1, 'people'], [1, 'us'], [1, 'country'], [0, 'fight'], [1, 'Trump'], [1, 'make'], [1, 'one'], [1, 'I’m'], [0, 'need'], [0, 'must'], [1, 'president'], [1, 'work'], [0, '#DemDebate'], [1, 'American'], [1, 'Americans'], [1, 'time'], [0, 'take'], [1, 'America'], [0, 'stand'], [0, 'care'], [1, 'This'], [0, '—'], [1, 'plan'], [1, 'world'], [1, 'House'], [1, 'health'], [1, 'like'], [1, 'President'], [1, 'every'], [1, 'change'], [0, 'rights'], [1, 'get'], [1, 'going'], [0, 'families'], [0, 'women'], [1, 'right'], [0, 'Donald'], [0, '@AyannaPressley'], [1, 'day'], [1, 'together'], [1, 'end'], [0, 'power'], [1, 'first'], [0, 'proud'], [0, 'protect'], [1, 'If'], [1, 'When'], [0, 'workers'], [1, 'In'], [0, 'violence'], [0, 'As'], [1, 'vote'], [1, \"I'm\"], [0, 'fighting'], [0, 'future'], [1, 'today'], [1, 'years'], [1, 'want'], [1, 'help'], [1, 'young'], [1, 'history'], [1, 'across'], [1, 'Today'], [1, 'campaign'], [0, 'Black'], [0, 'great'], [1, 'year'], [1, 'working'], [1, 'support'], [1, 'better'], [1, 'family'], [0, 'live'], [1, 'White'], [0, 'white'], [1, 'ensure'], [1, 'Our'], [1, 'It’s'], [1, 'law'], [0, 'big'], [1, 'build'], [1, 'win'], [0, 'climate'], [1, 'US'], [1, 'nation'], [1, 'know'], [0, 'union'], [1, 'got'], [1, 'justice'], [0, 'join'], [1, 'All'], [1, 'leaders'], [1, 'always'], [1, 'communities'], [0, 'way'], [1, 'it’s'], [1, 'see'], [0, 'class'], [1, 'public'], [1, 'Act'], [0, 'life']]\n",
      "Cornyn is a Republican, we are using sample labeling to test out our word frequency classifier.\n",
      "Probability:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Republican probability: [[1, 'people'], [1, 'US'], [1, 'This'], [1, 'Trump'], [0, 'Utah'], [1, 'great'], [0, 'We'], [1, 'Thank'], [1, 'House'], [1, 'Impeachment'], [1, 'work'], [1, 'want'], [0, '#openbordersinc'], [1, 'President'], [1, 'country'], [1, 'today'], [1, 'must'], [1, 'one'], [0, 'My'], [1, 'American'], [1, 'I’m'], [0, '@RobSchneider'], [1, 'vaping'], [1, 'years'], [0, 'Schiff'], [1, 'Democrats'], [1, 'public'], [0, '@dcexaminer'], [0, 'us'], [0, 'keep'], [1, 'state'], [1, 'like'], [1, 'young'], [1, 'know'], [1, 'back'], [1, 'right'], [1, 'new'], [1, 'last'], [1, 'hearing'], [1, 'need'], [1, 'A'], [1, 'Syria'], [1, 'Americans'], [1, 'Thanks'], [1, 'get'], [0, 'day'], [1, 'As'], [1, 'first'], [0, 'friend'], [0, 'debt'], [1, 'security'], [1, 'Iran'], [1, 'service'], [1, 'America'], [0, 'forward'], [0, 'colleagues'], [0, 'crisis'], [0, 'Utahns'], [0, 'many'], [0, 'future'], [0, 'decision'], [1, 'In'], [0, 'town'], [1, 'go'], [0, 'best'], [0, 'protect'], [1, 'support'], [1, 'national'], [1, 'working'], [1, 'time'], [1, 'important'], [0, 'Utah’s'], [0, 'well'], [1, 'help'], [1, 'It’s'], [0, 'workers'], [1, '1'], [0, 'good'], [0, 'thing'], [0, 'Great'], [1, 'No'], [0, '#AmericaFirst'], [1, 'Speaker'], [0, 'Biden'], [1, 'Congress'], [1, 'health'], [0, 'news'], [0, 'address'], [1, 'going'], [1, 'family'], [0, 'much'], [0, 'allies'], [1, 'year'], [1, 'high'], [0, 'night'], [0, '@Apple'], [0, '@sanctuary_no'], [0, '@JessicaV_CIS'], [0, 'Boston'], [1, 'impeachment'], [1, 'Ukraine']]\n",
      "Democratic probability: [[0, 'We'], [1, 'people'], [0, 'us'], [1, 'country'], [0, 'fight'], [1, 'Trump'], [0, 'make'], [1, 'one'], [1, 'I’m'], [1, 'need'], [1, 'must'], [1, 'president'], [1, 'work'], [0, '#DemDebate'], [1, 'American'], [1, 'Americans'], [1, 'time'], [0, 'take'], [1, 'America'], [0, 'stand'], [1, 'care'], [1, 'This'], [1, '—'], [0, 'plan'], [0, 'world'], [1, 'House'], [1, 'health'], [1, 'like'], [1, 'President'], [1, 'every'], [0, 'change'], [1, 'rights'], [1, 'get'], [1, 'going'], [1, 'families'], [0, 'women'], [1, 'right'], [0, 'Donald'], [0, '@AyannaPressley'], [0, 'day'], [0, 'together'], [1, 'end'], [0, 'power'], [1, 'first'], [1, 'proud'], [0, 'protect'], [1, 'If'], [1, 'When'], [0, 'workers'], [1, 'In'], [1, 'violence'], [1, 'As'], [1, 'vote'], [0, \"I'm\"], [1, 'fighting'], [0, 'future'], [1, 'today'], [1, 'years'], [1, 'want'], [1, 'help'], [1, 'young'], [0, 'history'], [1, 'across'], [1, 'Today'], [1, 'campaign'], [0, 'Black'], [1, 'great'], [1, 'year'], [1, 'working'], [1, 'support'], [0, 'better'], [1, 'family'], [0, 'live'], [1, 'White'], [0, 'white'], [0, 'ensure'], [0, 'Our'], [1, 'It’s'], [1, 'law'], [0, 'big'], [0, 'build'], [1, 'win'], [0, 'climate'], [1, 'US'], [1, 'nation'], [1, 'know'], [0, 'union'], [0, 'got'], [0, 'justice'], [0, 'join'], [1, 'All'], [0, 'leaders'], [0, 'always'], [0, 'communities'], [1, 'way'], [1, 'it’s'], [1, 'see'], [1, 'class'], [1, 'public'], [1, 'Act'], [0, 'life']]\n"
     ]
    }
   ],
   "source": [
    "#NEED TO TAKE THE DEMOCRAT AND REPUBLICAN TWEETS AND RELABEL THEIR MAX WORDS TO HOW MANY TIMES THEY APPEAR OVER ALL\n",
    "#THE TWEETS SO THAT WE CAN COMPARE THESE TO THAT. I THINK, NOT ENTIRELY SURE NEED TO THINK IT THROUGH.\n",
    "\n",
    "Hoyer = \"text_files/Hoyer_tweet_database.txt\"\n",
    "Buttigieg = \"text_files/Buttigieg_tweet_database.txt\"\n",
    "McCarthy = \"text_files/McCarthy_tweet_database.txt\"\n",
    "Cornyn = \"text_files/Cornyn_tweet_database.txt\"\n",
    "people = []\n",
    "repub_tweets = []\n",
    "demo_tweets = []\n",
    "people.append(Hoyer)\n",
    "people.append(Buttigieg)\n",
    "people.append(McCarthy)\n",
    "people.append(Cornyn)\n",
    "for element in people:\n",
    "    test_list = []\n",
    "    testing_list = []\n",
    "    tweet_list_test = []\n",
    "    file = open(element, \"r\")\n",
    "    if element[11] == \"H\":\n",
    "        print(\"Hoyer is a Democrat, we are using sample labeling to test out our word frequency classifier.\")\n",
    "    elif element[11] == \"B\":\n",
    "        print(\"Buttigieg is a Democrat, we are using sample labeling to test out our word frequency classifier.\")\n",
    "    elif element[11] == \"M\":\n",
    "        print(\"McCarthy is a Republican, we are using sample labeling to test out our word frequency classifier.\")\n",
    "    else:\n",
    "        print(\"Cornyn is a Republican, we are using sample labeling to test out our word frequency classifier.\")\n",
    "    lines = file.readlines()\n",
    "    for line in lines:\n",
    "        test_list.append(line)\n",
    "#     print(test_list)\n",
    "\n",
    "    stoplist = stopwords.words('english')\n",
    "    delimeters = \",./)(:;\\n-!\"\n",
    "    otherTokens= [\"yes\", \"okay\", \"ok\", \",\", \";\", \":\", \".\", \"?\", '”', \"--\", \"!\", \"I\", \"n't\", \"would\", \"could\", '“',\n",
    "                 \"Dr.\", \"Mrs.\", \"And\", \"But\", \"Miss\", \"'s\", \"It\", \"The\", \"He\", \"You\", \"/\", \")\", \"(\", \"\\n\", \"\", \"&amp\"]\n",
    "    for x in range(len(otherTokens)):\n",
    "        stoplist.append(otherTokens[x])\n",
    "\n",
    "    for tweet in test_list:\n",
    "        testing_list = []\n",
    "        for i in delimeters: \n",
    "            if i ==\"\\n\" or i == \"-\":\n",
    "                tweet = tweet.replace(i , \" \")\n",
    "            else: \n",
    "                tweet = tweet.replace(i , \"\")\n",
    "        list_tweet_words= tweet.split(\" \")\n",
    "        for i in list_tweet_words:\n",
    "            if \"http\" not in i and i not in stoplist:# and i not in testing_list:\n",
    "                testing_list.append(i) \n",
    "        tweet_list_test.append(testing_list)\n",
    "#     print(testing_list)\n",
    "#     print(tweet_list_test)\n",
    "\n",
    "    demo_prob = demo_max_words_weight_list\n",
    "    repub_prob = repub_max_words_weight_list\n",
    "    for element in demo_prob:\n",
    "        element[0] = 0\n",
    "    for element in repub_prob:\n",
    "        element[0] = 0\n",
    "    print(\"Probability:\")\n",
    "    for element in tweet_list_test:\n",
    "        for w in element:\n",
    "            for word in repub_prob: \n",
    "                if w == word[1]:\n",
    "                    #If want counts do this\n",
    "#                     word[0] +=1\n",
    "                    #If want just occurrences do this\n",
    "                    word[0] = 1\n",
    "            for word in demo_prob:\n",
    "                if w == word[1]:\n",
    "                    #If want counts do this\n",
    "#                     word[0] +=1\n",
    "                    #If want just occurrences do this\n",
    "                    word[0] = 1\n",
    "    print(\"Republican probability:\", repub_prob)\n",
    "    print(\"Democratic probability:\", demo_prob)\n",
    "    temp_demo = []\n",
    "    temp_repub = []\n",
    "    for i in range(len(repub_prob)):\n",
    "        temp_demo.append(demo_prob[i][0])\n",
    "        temp_repub.append(repub_prob[i][0])\n",
    "    demo_tweets.append(temp_demo)\n",
    "    repub_tweets.append(temp_repub)\n",
    "#     if demo_prob > repub_prob:\n",
    "#         print(\"This tweet was more likely posted by a Democrat\")\n",
    "#     else:\n",
    "#         print(\"This tweet was more likely posted by a Republican\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Republican Vector: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Democratic Vector: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Republican Tweet Test Vectors: [[1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1], [1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0], [1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1], [1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1]]\n",
      "Democratic Tweet Test Vectors: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1], [1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0], [0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0]]\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from numpy import reshape\n",
    "import numpy as np\n",
    "demo_test = [1 for x in range(len(demo_prob))]\n",
    "repub_test = [1 for x in range(len(repub_prob))]\n",
    "print(\"Republican Vector:\", repub_test)\n",
    "print(\"Democratic Vector:\", demo_test)\n",
    "print(\"Republican Tweet Test Vectors:\", repub_tweets)\n",
    "print(\"Democratic Tweet Test Vectors:\", demo_tweets)\n",
    "model = GaussianNB()\n",
    "a = np.array(demo_test).reshape(-1,1)\n",
    "b = np.array(demo_tweets[2]).reshape(-1,1)\n",
    "model.fit(b, a)\n",
    "score = model.score(b, a)\n",
    "print(str(score*100) + \"%\")\n",
    "#THE SCORE IS WRONG BUT WHY????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "need, care, it’s, new, president, \n",
      "\n",
      "people, white, time, black, must, \n",
      "\n",
      "health, care, people, protect, #demdebate, \n",
      "\n",
      "stand, we’re, going, president, i’m, \n",
      "\n",
      "people, stand, fight, must, that’s, \n",
      "\n",
      "fight, country, i'm, going, great, \n",
      "\n",
      "—, women, it’s, power, trump, \n",
      "\n",
      "&amp;, fight, end, people, work, \n",
      "\n",
      "white, trump, fight, donald, @ayannapressley, \n",
      "\n",
      "i’m, work, president, trump, great, \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import gensim\n",
    "stoplist = stopwords.words('english')\n",
    "stoplist.extend([\"ever\", \"one\", \"do\",\"does\",\"make\", \"go\", \"us\", \"to\", \"get\", \"about\", \"may\", \"s\", \".\", \",\", \"!\", \"i\", \"I\", '\\\"', \"?\", \";\", \"--\", \"--\", \"would\", \"could\", \"”\", \"Mr.\", \"Miss\", \"Mrs.\", \"don’t\", \"said\", \"can't\", \"didn't\", \"aren't\", \"I'm\", \"you're\", \"they're\", \"'s\"])\n",
    "stoplist.remove(\"which\")\n",
    "alltokens = []\n",
    "f = open(\"text_files/democrat_tweet_database.txt\", \"r\")\n",
    "for line in f:\n",
    "    line = line.rstrip()\n",
    "    line = re.sub(r\"(^| )[0-9]+($| )\", r\" \", line)\n",
    "    addme = [t.lower() for t in line.split() if t.lower() not in stoplist]\n",
    "    alltokens.append(addme)\n",
    "    \n",
    "f.close()\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(alltokens)\n",
    "\n",
    "corpus = [dictionary.doc2bow(alltok) for alltok in alltokens]\n",
    "\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=10, id2word = dictionary, passes=10)\n",
    "\n",
    "for i in range(10):\n",
    "    words = ldamodel.show_topic(i, 5)\n",
    "    for w in words:\n",
    "        print(w[0], end=\", \")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'GoogleNews-vectors-negative300-SLIM.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-45c9b8a61222>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbigmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GoogleNews-vectors-negative300-SLIM.bin\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"big model loaded!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1496\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1497\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1498\u001b[0;31m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[1;32m   1499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading projection weights from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m         \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# throws for invalid file format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, transport_params)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m     )\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, ignore_ext, buffering, encoding, errors)\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPY3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'GoogleNews-vectors-negative300-SLIM.bin'"
     ]
    }
   ],
   "source": [
    "bigmodel = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300-SLIM.bin\", binary=True)\n",
    "print(\"big model loaded!\")\n",
    "\n",
    "\n",
    "\n",
    "# Some words associated with 2 different categories: work and school\n",
    "\n",
    "vecwords1 = \" make sure our criminal justice system treats everyone equally under law strengthen laws that protect women in workplace from harassment make sure they are paid equally When we have been at such crossroads before Americans have made right choice Not because we sat back waited for history happen but because we marched mobilized voted We made history happen Tomorrow elections might be most important our lifetimes health care millions is on ballot Making sure working families get fair shake is on ballot character our country is on ballot Speaking preexisting conditions open enrollment starts today\".split()  \n",
    "\n",
    "vecwords2 = \"We have got build stronger more inclusive middle class That starts strengthening public private sector unions all workers should be empowered bargain for what they deserve at I am much richer than people even thought that is good thing Jobs Jobs Jobs Bob Mueller after spending two years million dollars went over all my financials my taxes found nothing Now Witch Hunt continues with local New York Democrat prosecutors going over every financial deal I have ever done\".split()\n",
    "vecs = []\n",
    "vecwords = []\n",
    "\n",
    "# Get their vectors\n",
    "for w in vecwords1:\n",
    "    v = bigmodel[w]\n",
    "    vecs.append(v)\n",
    "    vecwords.append(w)\n",
    "\n",
    "for w in vecwords2:\n",
    "    v = bigmodel[w]\n",
    "    vecs.append(v)\n",
    "    vecwords.append(w)\n",
    "\n",
    "    \n",
    "#tsne = TSNE(n_components=2, random_state=0)\n",
    "#vectors2d = tsne.fit_transform(vecs)\n",
    "\n",
    "# Do the PCA to reduce to 2 dimensions\n",
    "pca = PCA(n_components=2, whiten=True)\n",
    "vectors2d = pca.fit(vecs).transform(vecs)\n",
    "\n",
    "# Again, ugly matplotlib code to create visualization\n",
    "i = 0\n",
    "for point, word in zip(vectors2d, vecwords):\n",
    "    if i < len(vecwords1):\n",
    "        plt.scatter(point[0], point[1], c='r')\n",
    "    else:\n",
    "        plt.scatter(point[0], point[1], c='b')\n",
    "    i += 1\n",
    "    \n",
    "    plt.annotate(\n",
    "            word, \n",
    "            xy=(point[0], point[1]),\n",
    "            xytext=(7, 6),\n",
    "            textcoords='offset points',\n",
    "            ha='left' ,\n",
    "            va='top',\n",
    "            size=\"medium\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
